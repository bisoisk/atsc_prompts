{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"lm_further_pretraining_gpt-2_amazon_electronics.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3GvsqHtXQvvX"},"source":["# Initial Setups\n"]},{"cell_type":"markdown","metadata":{"id":"MTrRCzgmFwYn"},"source":["## (Google Colab use only)"]},{"cell_type":"code","metadata":{"id":"xC1S4mWFFv5U"},"source":["# Use Google Colab\n","use_colab = True\n","\n","# Is this notebook running on Colab?\n","# If so, then google.colab package (github.com/googlecolab/colabtools)\n","# should be available in this environment\n","\n","# Previous version used importlib, but we could do the same thing with\n","# just attempting to import google.colab\n","try:\n","    from google.colab import drive\n","    colab_available = True\n","except:\n","    colab_available = False\n","\n","if use_colab and colab_available:\n","    drive.mount('/content/drive')\n","    \n","    # If there's a package I need to install separately, do it here\n","    #!pip install pyro-ppl\n","\n","    # cd to the appropriate working directory under my Google Drive\n","    %cd 'drive/My Drive/cs696ds_lexalytics/Language Model Finetuning'\n","    \n","    # List the directory contents\n","    !ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZ5SZXkbEp34"},"source":["## Experiment ID\n","\n","**NOTE**: The following `experiment_id` MUST BE CHANGED in order to avoid overwriting the files from other experiments!!!!!!"]},{"cell_type":"code","metadata":{"id":"rUqrV6VeEs3Z"},"source":["# We will use the following string ID to identify this particular (training) experiments\n","# in directory paths and other settings\n","experiment_id = 'lm_further_pretraining_gpt-2_amazon_electronics'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ot4PHli9-Kou"},"source":["## Package Install"]},{"cell_type":"code","metadata":{"id":"uCkykoLX4CXF"},"source":["# Install packages specified in requirements\n","!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0hlNdKpPG-L4"},"source":["# NVidia APEX install\n","!unzip -o apex-master_downloaded_01Mar2021.zip\n","%cd apex-master\n","!pip install -v --no-cache-dir ./\n","%cd .."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgITVrTUZstB"},"source":["# IPython reloading magic\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kSItEk35-R8o"},"source":["## Package Imports"]},{"cell_type":"code","metadata":{"id":"o-jSRWQfLL4U"},"source":["import sys\n","import os\n","import random\n","import numpy as np\n","import torch\n","import transformers\n","import datasets\n","\n","import utils\n","\n","# Random seed settings\n","random_seed = 696\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","\n","# Print version information\n","print(\"Python version: \" + sys.version)\n","print(\"NumPy version: \" + np.__version__)\n","print(\"PyTorch version: \" + torch.__version__)\n","print(\"Transformers version: \" + transformers.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rkKDoXUp-UIi"},"source":["## PyTorch GPU settings"]},{"cell_type":"code","metadata":{"id":"je9BT2pQIpUx"},"source":["if torch.cuda.is_available():\n","    torch_device = torch.device('cuda')\n","\n","    # Set this to True to make your output immediately reproducible\n","    # Note: https://pytorch.org/docs/stable/notes/randomness.html\n","    torch.backends.cudnn.deterministic = False\n","    \n","    # Disable 'benchmark' mode: Set this False if you want to measure running times more fairly\n","    # Note: https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n","    torch.backends.cudnn.benchmark = True\n","    \n","    # Faster Host to GPU copies with page-locked memory\n","    use_pin_memory = True \n","\n","    # CUDA libraries version information\n","    print(\"CUDA Version: \" + str(torch.version.cuda))\n","    print(\"cuDNN Version: \" + str(torch.backends.cudnn.version()))\n","    print(\"CUDA Device Name: \" + str(torch.cuda.get_device_name()))\n","    print(\"CUDA Capabilities: \"+ str(torch.cuda.get_device_capability()))\n","else:\n","    torch_device = torch.device('cpu')\n","    use_pin_memory = False\n","\n","print()\n","print(\"PyTorch device selected:\", torch_device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H3txs5s7Q1UG"},"source":["# Further pre-training"]},{"cell_type":"markdown","metadata":{"id":"gEnUsBDUOLAm"},"source":["## Load the GPT-2 model"]},{"cell_type":"code","metadata":{"id":"W3TCqS-3OOIj"},"source":["tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\", cache_dir='./gpt2_cache')\n","model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir='./gpt2_cache')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PrLTLsRFRUKK"},"source":["## Load the Amazon electronics dataset"]},{"cell_type":"code","metadata":{"id":"ZpKL0urORmkm"},"source":["amazon = datasets.load_dataset(\n","    './dataset_scripts/amazon_ucsd_reviews',\n","    data_files={\n","        'train': 'dataset_files/amazon_ucsd_reviews/Electronics.json.gz',\n","    },\n","    cache_dir='./dataset_cache')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2m9wZvndVCvy"},"source":["data_amazon_train = amazon['train']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WrB-ZoTz5-zb"},"source":["print(\"Number of training data:\", len(data_amazon_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQ_RaowTYC9X"},"source":["# Check out how individual data points look like\n","print(data_amazon_train[696])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WqyEwcDGSJjs"},"source":["### Preprocessing: Encode the text with Tokenizer"]},{"cell_type":"code","metadata":{"id":"tTVxGSY4oe7e"},"source":["train_dataset_pretraining = data_amazon_train.map(\n","    lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=256),\n","    remove_columns=data_amazon_train.column_names,\n","    batched=True, num_proc=16)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p1N4PUdHZeIm"},"source":["## Pre-train further"]},{"cell_type":"markdown","metadata":{"id":"PD6lfHh8mURq"},"source":["### Training settings"]},{"cell_type":"code","metadata":{"id":"RFho5vyJgVaj"},"source":["# CLM\n","collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ps4XzQxmTgS"},"source":["training_args = transformers.TrainingArguments(\n","    output_dir=os.path.join('.', 'progress', experiment_id, 'results'), # output directory\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,              # total number of training epochs\n","    per_device_train_batch_size=32,  # 64 * 2 GPUs = 128 Total\n","    evaluation_strategy='epoch',\n","    warmup_steps=5000,               # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir=os.path.join('.', 'progress', experiment_id, 'logs'), # directory for storing logs\n","    seed=random_seed,\n","    fp16=True,\n","    fp16_opt_level='O2',\n","    prediction_loss_only=True,\n","    load_best_model_at_end=True,\n","    dataloader_num_workers=22,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQsRp9lMZstE"},"source":["print(training_args.n_gpu)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OQJ8IOIYHXb"},"source":["trainer = transformers.Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=collator, # do the masking on the go\n","    train_dataset=train_dataset_pretraining,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZTx5562OmXx5"},"source":["### Training loop"]},{"cell_type":"code","metadata":{"id":"-_v3lAPvb9DK"},"source":["%%time\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wLj4Ico8vwhO"},"source":["### Save the model to the local directory"]},{"cell_type":"code","metadata":{"id":"WLViPwdTvvxP"},"source":["trainer.save_model(os.path.join('.', 'trained_models', experiment_id))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIQn4r1oVJp6"},"source":["tokenizer.save_pretrained(os.path.join('.', 'trained_models', experiment_id))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNUCURCduqYa"},"source":["## LM Evaluation"]},{"cell_type":"code","metadata":{"id":"u1XN8ot3us18"},"source":["eval_results = trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKkBZEpF07Ip"},"source":["print(eval_results)\n","\n","perplexity = np.exp(eval_results[\"eval_loss\"])\n","\n","print(perplexity)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_bAx1rL0KFb"},"source":["## Playing with my own input sentences"]},{"cell_type":"code","metadata":{"id":"hmIJszldveH6"},"source":["example = f\"\"\"The {tokenizer.mask_token} of {tokenizer.mask_token} is awful, but its {tokenizer.mask_token} is fantastic.\"\"\"\n","\n","example_encoded = tokenizer.encode(example, add_special_tokens=True, return_tensors=\"pt\").to(torch_device)\n","\n","# Let's decode this back just to see how they were actually encoded\n","example_tokens = []\n","\n","for id in example_encoded[0]:\n","    example_tokens.append(tokenizer.convert_ids_to_tokens(id.item()))\n","\n","print(example_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CbHup4-rxBSD"},"source":["example_prediction = model(example_encoded)\n","\n","example_prediction_argmax = torch.argmax(example_prediction[0], dim=-1)[0]\n","\n","print(example_prediction_argmax)\n","\n","print(tokenizer.decode(example_prediction_argmax))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EM3YetZAm3L-"},"source":[""],"execution_count":null,"outputs":[]}]}