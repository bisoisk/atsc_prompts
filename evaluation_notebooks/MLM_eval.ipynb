{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval template for MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to pretrained MLM model folder or the string \"bert-base-uncased\"\n",
    "bert_model_path = '../trained_models/lm_further_pretraining_bert_amazon_electronics'\n",
    "\n",
    "#in domain will be used for testing and classification model validation\n",
    "#out domain will be used to train the classifier\n",
    "#Values are \"Laptop\" or \"Restaurant\"\n",
    "in_domain_dataset_name = \"Laptops\"\n",
    "out_domain_dataset_name = \"Restaurants\"\n",
    "\n",
    "#Prompts to be added to the end of each review text\n",
    "sentiment_prompts = [\n",
    "    \"The {aspect} is [MASK].\",\n",
    "    \"I [MASK] the {aspect}.\",\n",
    "    \"I felt the {aspect} was [MASK]\",\n",
    "    \"The {aspect} made me feel [MASK]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../trained_models/lm_further_pretraining_bert_amazon_electronics and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir='../bert_base_cache')\n",
    "\n",
    "#Loads pretrained bert model as a normal bert model\n",
    "model = transformers.BertModel.from_pretrained(pretrained_model_name_or_path=bert_model_path, cache_dir='../bert_base_cache')\n",
    "\n",
    "#Freezes all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#Send model to gpu\n",
    "model = model.to(device=device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SemEval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3a5e3b6e10fdd547\n",
      "Reusing dataset sem_eval2014_task4_dataset (../dataset_cache\\sem_eval2014_task4_dataset\\default-3a5e3b6e10fdd547\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969)\n",
      "Using custom data configuration default-790c778f2f732468\n",
      "Reusing dataset sem_eval2014_task4_dataset (../dataset_cache\\sem_eval2014_task4_dataset\\default-790c778f2f732468\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aspect': 'Boot time', 'sentiment': 0, 'text': 'Boot time is super fast, around anywhere from 35 seconds to 1 minute.'}\n"
     ]
    }
   ],
   "source": [
    "#Load semeval for both domains\n",
    "in_domain_semeval_dataset = datasets.load_dataset(\n",
    "    os.path.abspath('../dataset_scripts/semeval2014_task4/semeval2014_task4.py'),\n",
    "    data_files={\n",
    "        'test': os.path.abspath('../dataset_files/semeval_2014/{domain}_Test_Gold.xml'.format(domain=in_domain_dataset_name)),\n",
    "        'train': os.path.abspath('../dataset_files/semeval_2014/{domain}_Train_v2.xml'.format(domain=in_domain_dataset_name))\n",
    "    },\n",
    "    cache_dir='../dataset_cache')\n",
    "\n",
    "\n",
    "out_domain_semeval_dataset = datasets.load_dataset(\n",
    "    os.path.abspath('../dataset_scripts/semeval2014_task4/semeval2014_task4.py'),\n",
    "    data_files={\n",
    "        'test': os.path.abspath('../dataset_files/semeval_2014/{domain}_Test_Gold.xml'.format(domain=out_domain_dataset_name)),\n",
    "        'train': os.path.abspath('../dataset_files/semeval_2014/{domain}_Train_v2.xml'.format(domain=out_domain_dataset_name))\n",
    "    },\n",
    "    cache_dir='../dataset_cache')\n",
    "\n",
    "train_dataset = out_domain_semeval_dataset[\"train\"]\n",
    "val_dataset = in_domain_semeval_dataset[\"train\"]\n",
    "test_dataset = in_domain_semeval_dataset[\"test\"]\n",
    "\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding prompts to review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in batches from the dataset and makes an example for every prompt, text pair\n",
    "def add_prompts(reviews, prompts):\n",
    "\n",
    "    #Collect the output from each example in the batch\n",
    "    texts = []\n",
    "    sentiments = []\n",
    "    ids = []\n",
    "    aspect_prompts = []\n",
    "    \n",
    "    for i in range(len(reviews[\"aspect\"])):\n",
    "        \n",
    "        aspect = reviews[\"aspect\"][i]\n",
    "        text = reviews[\"text\"][i]\n",
    "        sentiment = reviews[\"sentiment\"][i]\n",
    "        \n",
    "        #ID to identify the review, aspect pair for regrouping later\n",
    "        review_aspect_id = str(uuid.uuid1())\n",
    "        \n",
    "        for p in prompts:\n",
    "            aspect_prompt = p.format(aspect=aspect)\n",
    "\n",
    "            texts.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "            ids.append(review_aspect_id)\n",
    "            aspect_prompts.append(aspect_prompt)\n",
    "\n",
    "    return {\"text\":texts, \"prompt\": aspect_prompts, \"label\": sentiments, \"review_aspect_id\": ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../dataset_cache\\sem_eval2014_task4_dataset\\default-790c778f2f732468\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969\\cache-6268c5a73dc375bd.arrow\n",
      "Loading cached processed dataset at ../dataset_cache\\sem_eval2014_task4_dataset\\default-3a5e3b6e10fdd547\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969\\cache-3c33b4c07a4b0f9e.arrow\n",
      "Loading cached processed dataset at ../dataset_cache\\sem_eval2014_task4_dataset\\default-3a5e3b6e10fdd547\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969\\cache-acaa1f1ef2d50386.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'prompt': 'The Boot time is [MASK].', 'review_aspect_id': 'b052c13b-82b9-11eb-8344-7085c2c04498', 'text': 'Boot time is super fast, around anywhere from 35 seconds to 1 minute.'}\n",
      "2552\n"
     ]
    }
   ],
   "source": [
    "#Map to add_prompts\n",
    "train_prompt_dataset = train_dataset.map(\n",
    "    lambda e: add_prompts(e, sentiment_prompts),\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batched=True)\n",
    "\n",
    "val_prompt_dataset = val_dataset.map(\n",
    "    lambda e: add_prompts(e, sentiment_prompts),\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    batched=True)\n",
    "\n",
    "test_prompt_dataset = test_dataset.map(\n",
    "    lambda e: add_prompts(e, sentiment_prompts),\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    batched=True)\n",
    "\n",
    "print(test_prompt_dataset[0])\n",
    "print(len(test_prompt_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize every example and run it through the bert model\n",
    "#Outputs the last hidden state of the model\n",
    "def run_model(reviews, tokenizer, model, device):\n",
    "    \n",
    "    #Only the review text is truncated so that the mask token always exists in the tokens\n",
    "    batch_tokens = tokenizer(reviews[\"text\"], reviews[\"prompt\"], \n",
    "                             truncation='only_first', padding='max_length', max_length=256, return_tensors=\"pt\")\n",
    "    batch_tokens.to(device=device)\n",
    "    \n",
    "    \n",
    "    #Figures out where the mask token was placed\n",
    "    masked_indexes = []\n",
    "\n",
    "    for tokens_input_ids in batch_tokens.data[\"input_ids\"]:\n",
    "\n",
    "        masked_index = torch.nonzero(tokens_input_ids == tokenizer.mask_token_id, as_tuple=False).item()\n",
    "        masked_indexes.append(masked_index)\n",
    "    \n",
    "    #Run the batch through the model\n",
    "    outputs = model(**batch_tokens)\n",
    "    \n",
    "    #Extracts the last hidden states from the batch output\n",
    "    output_list = []\n",
    "    for i in range(len(outputs[\"last_hidden_state\"])):\n",
    "        masked_index = masked_indexes[i]\n",
    "        output_list.append(outputs[\"last_hidden_state\"][i][masked_index])\n",
    "    \n",
    "    return {\"hidden_state\":output_list, \"label\": reviews[\"label\"], \"review_aspect_id\": reviews[\"review_aspect_id\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799a9a16232445e09242864908de1904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3602.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Maps the dataset using run_model\n",
    "train_model_output = train_prompt_dataset.map(\n",
    "    lambda e: run_model(e, tokenizer, model, device),\n",
    "    remove_columns=train_prompt_dataset.column_names,\n",
    "    batched=True, batch_size=4, num_proc=None)\n",
    "\n",
    "val_model_output = val_prompt_dataset.map(\n",
    "    lambda e: run_model(e, tokenizer, model, device),\n",
    "    remove_columns=train_prompt_dataset.column_names,\n",
    "    batched=True, batch_size=4, num_proc=None)\n",
    "\n",
    "test_model_output = test_prompt_dataset.map(\n",
    "    lambda e: run_model(e, tokenizer, model, device),\n",
    "    remove_columns=train_prompt_dataset.column_names,\n",
    "    batched=True, batch_size=4, num_proc=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regroups the outputs by review, aspect pairs and concats the outputs into one long tensor\n",
    "def concat_tensors(examples, device):\n",
    "    #Make sure that everything in the batch is from the same review, aspect pair\n",
    "    ids = examples[\"review_aspect_id\"]\n",
    "    labels = examples[\"label\"]\n",
    "    \n",
    "    for a in ids:\n",
    "        for b in ids:\n",
    "            assert a == b\n",
    "            \n",
    "    for a in labels:\n",
    "        for b in labels:\n",
    "            assert a == b\n",
    "    \n",
    "    hidden_state_tensors = torch.FloatTensor(examples[\"hidden_state\"]).to(device=device)\n",
    "    cat_hs_tensor = torch.cat(tuple(hidden_state_tensors), 0).to(device=device)\n",
    "    \n",
    "    return {\"hidden_state\": [cat_hs_tensor], \"label\": [examples[\"label\"][0]], \"review_aspect_id\": [examples[\"review_aspect_id\"][0]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map to concat_tensors\n",
    "#Batch size has to be the same as sentiment prompts so that the examples in a batches all come from the same review\n",
    "train_hs_dataset = train_model_output.map(\n",
    "    lambda e: concat_tensors(e, device),\n",
    "    remove_columns=train_model_output.column_names,\n",
    "    batched=True, batch_size=len(sentiment_prompts))\n",
    "\n",
    "val_hs_dataset = val_model_output.map(\n",
    "    lambda e: concat_tensors(e, device),\n",
    "    remove_columns=val_model_output.column_names,\n",
    "    batched=True, batch_size=len(sentiment_prompts))\n",
    "\n",
    "test_hs_dataset = test_model_output.map(\n",
    "    lambda e: concat_tensors(e, device),\n",
    "    remove_columns=test_model_output.column_names,\n",
    "    batched=True, batch_size=len(sentiment_prompts))\n",
    "\n",
    "#print(train_hs_dataset[0])\n",
    "print(len(train_hs_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification code adapted from : https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the classification model that was trained to convert hidden state values to a class prediction\n",
    "class SentimentClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output dataset to tensors from a dataloader\n",
    "train_hs_dataset.set_format(type='torch', columns=['hidden_state', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(train_hs_dataset, batch_size=32)\n",
    "\n",
    "val_hs_dataset.set_format(type='torch', columns=['hidden_state', 'label'])\n",
    "val_dataloader = torch.utils.data.DataLoader(val_hs_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train classifier\n",
    "epochs = 10\n",
    "\n",
    "#Bert hidden State size\n",
    "input_dim = 768 * len(sentiment_prompts)\n",
    "print(input_dim)\n",
    "output_dim = 3\n",
    "lr_rate = 0.0001\n",
    "\n",
    "classifier_model = SentimentClassifier(input_dim, output_dim)\n",
    "classifier_model.to(device=device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier_model.parameters(), lr=lr_rate)\n",
    "\n",
    "for epoch in range(int(epochs)):\n",
    "    for batch in dataloader:\n",
    "        hidden_states = batch[\"hidden_state\"]\n",
    "        hidden_states = hidden_states.float().to(device=device)\n",
    "        \n",
    "        labels = batch[\"label\"]\n",
    "        labels = labels.to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_model(hidden_states)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Validate the model using val dataset\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_val in val_dataloader:\n",
    "\n",
    "        hidden_states = batch_val[\"hidden_state\"]\n",
    "        hidden_states = hidden_states.float().to(device=device)\n",
    "\n",
    "        labels = batch_val[\"label\"]\n",
    "        labeles = labels.to(device=device)\n",
    "\n",
    "        outputs = classifier_model(hidden_states)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total+= labels.size(0)\n",
    "        # for gpu, bring the predicted and labels back to cpu for python operations to work\n",
    "        predicted = predicted.to(device=\"cpu\")\n",
    "        labels = labels.to(device=\"cpu\")\n",
    "\n",
    "\n",
    "        correct+= (predicted == labels).sum()\n",
    "    accuracy = 100 * correct/total\n",
    "    print(\"Epoch: {}. Loss: {}. Validation Accuracy: {}.\".format(epoch, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifier on hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.eval()\n",
    "test_hs_dataset.set_format(type='torch', columns=['hidden_state', 'label'])\n",
    "\n",
    "#Run the LR model on the dataset\n",
    "def make_predictions(examples, classifier_model, device):\n",
    "    \n",
    "    cat_hs_tensor = examples[\"hidden_state\"].float().to(device=device)\n",
    "    class_probs = classifier_model(cat_hs_tensor)\n",
    "\n",
    "    predictions = []\n",
    "    for cp in class_probs:\n",
    "        predictions.append(torch.argmax(cp))\n",
    "    \n",
    "    return {\"prediction\": predictions, \"label\": examples[\"label\"].tolist()}\n",
    "\n",
    "#Batch size has to be the same as sentiment prompts so that the examples in a batches all come from the same review\n",
    "predictions = test_hs_dataset.map(\n",
    "    lambda e: make_predictions(e, classifier_model, device),\n",
    "    batched=True, batch_size=7, num_proc=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert prompt class probs to predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This regroups the models by the review_aspect_id\n",
    "#and makes a prediction based upon each groups class prob output from the LR model\n",
    "def make_predictions(example, device):\n",
    "    ids = example[\"review_aspect_id\"]\n",
    "    labels = example[\"label\"]\n",
    "    for a in ids:\n",
    "        for b in ids:\n",
    "            assert a == b\n",
    "            \n",
    "    for a in labels:\n",
    "        for b in labels:\n",
    "            assert a == b\n",
    "            \n",
    "    class_probs = torch.FloatTensor(example[\"class_probs\"])\n",
    "    prediction_mean = torch.mean(class_probs, 0)\n",
    "    prediction = torch.argmax(prediction_mean)\n",
    "    \n",
    "    return {\"prediction\": [prediction], \"label\": [example[\"label\"][0]], \"review_aspect_id\": [example[\"review_aspect_id\"][0]]}\n",
    "\n",
    "predictions = LR_output.map(\n",
    "    lambda e: make_predictions(e, cuda),\n",
    "    remove_columns=LR_output.column_names,\n",
    "    batched=True, batch_size=len(sentiment_prompts), num_proc=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate metrics and confusion matrix based upon predictions and true labels\n",
    "accuracy = accuracy_score(predictions[\"label\"], predictions[\"prediction\"])\n",
    "\n",
    "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
    "\n",
    "cm = confusion_matrix(predictions[\"label\"], predictions[\"prediction\"])\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"positive\", \"negative\", \"neutral\"]],\n",
    "                  columns = [i for i in [\"positive\", \"negative\", \"neutral\"]])\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "ax = sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
