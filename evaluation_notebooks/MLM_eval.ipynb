{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf3lVTZYhbzA"
   },
   "source": [
    "# Initial Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ORFXeezn5Og"
   },
   "source": [
    "## (Google Colab use only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFAQ6IgXn8FK"
   },
   "outputs": [],
   "source": [
    "# Use Google Colab\n",
    "use_colab = True\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd 'drive/My Drive/zero_shot_atsc'\n",
    "    \n",
    "    # Install packages specified in requirements\n",
    "    !pip install -r requirements.txt\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgzsHF7Zhbzo"
   },
   "source": [
    "## Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUpGBmOJhbzs"
   },
   "outputs": [],
   "source": [
    "#path to pretrained MLM model folder or the string \"bert-base-uncased\"\n",
    "bert_model_path = '../trained_models/lm_further_pretraining_bert_amazon_electronics'\n",
    "\n",
    "#in domain will be used for testing and classification model validation\n",
    "#out domain will be used to train the classifier\n",
    "#Values are \"Laptop\" or \"Restaurant\"\n",
    "in_domain_dataset_name = \"Laptops\"\n",
    "out_domain_dataset_name = \"Restaurants\"\n",
    "\n",
    "#Prompts to be added to the end of each review text\n",
    "sentiment_prompts = [\n",
    "    \"The {aspect} is [MASK].\",\n",
    "    \"I [MASK] the {aspect}.\",\n",
    "    \"I felt the {aspect} was [MASK]\",\n",
    "    \"The {aspect} made me feel [MASK]\"]\n",
    "\n",
    "# Random seed\n",
    "random_seed = 696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYZesqTioMvF"
   },
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlK_-DrWhbzb",
    "outputId": "87c5bf43-a241-46e3-a4eb-9ec8b4307f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Random seed settings\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWuR30eUoTWP"
   },
   "source": [
    "## PyTorch GPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfNlm-ykoSlM"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    torch_device = torch.device('cuda')\n",
    "\n",
    "    # Set this to True to make your output immediately reproducible\n",
    "    # Note: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    \n",
    "    # Disable 'benchmark' mode: Set this False if you want to measure running times more fairly\n",
    "    # Note: https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Faster Host to GPU copies with page-locked memory\n",
    "    use_pin_memory = True\n",
    "    \n",
    "    # Number of compute devices to be used for training\n",
    "    training_device_count = torch.cuda.device_count()\n",
    "\n",
    "    # CUDA libraries version information\n",
    "    print(\"CUDA Version: \" + str(torch.version.cuda))\n",
    "    print(\"cuDNN Version: \" + str(torch.backends.cudnn.version()))\n",
    "    print(\"CUDA Device Name: \" + str(torch.cuda.get_device_name()))\n",
    "    print(\"CUDA Capabilities: \"+ str(torch.cuda.get_device_capability()))\n",
    "    print(\"Number of CUDA devices: \"+ str(training_device_count))\n",
    "    \n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "    use_pin_memory = False\n",
    "    \n",
    "    # Number of compute devices to be used for training\n",
    "    training_device_count = 1\n",
    "\n",
    "print()\n",
    "print(\"PyTorch device selected:\", torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayX5VRLfocFk"
   },
   "source": [
    "# Prepare training data for prompt-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jNAtuv-hbzv"
   },
   "source": [
    "## Load the pretrained LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "En2BmfjVhbzy",
    "outputId": "c322b655-8eb4-489f-ab1b-f0ec1356b5c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../trained_models/lm_further_pretraining_bert_amazon_electronics and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir='../bert_base_cache')\n",
    "\n",
    "# Loads pretrained bert model as a normal bert model\n",
    "model = transformers.BertModel.from_pretrained(pretrained_model_name_or_path=bert_model_path, cache_dir='../bert_base_cache')\n",
    "\n",
    "# Freezes all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Send model to gpu\n",
    "model = model.to(device=torch_device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9LAAJP-hbz7"
   },
   "source": [
    "## Load the SemEval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpL2uHPUhbz9",
    "outputId": "be8ffbc9-7d25-461d-cb50-f9f16c65c174"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3a5e3b6e10fdd547\n",
      "Reusing dataset sem_eval2014_task4_dataset (../dataset_cache\\sem_eval2014_task4_dataset\\default-3a5e3b6e10fdd547\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969)\n",
      "Using custom data configuration default-790c778f2f732468\n",
      "Reusing dataset sem_eval2014_task4_dataset (../dataset_cache\\sem_eval2014_task4_dataset\\default-790c778f2f732468\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aspect': 'Boot time', 'sentiment': 0, 'text': 'Boot time is super fast, around anywhere from 35 seconds to 1 minute.'}\n"
     ]
    }
   ],
   "source": [
    "# Load semeval for both domains\n",
    "in_domain_semeval_dataset = datasets.load_dataset(\n",
    "    os.path.abspath('../dataset_scripts/semeval2014_task4/semeval2014_task4.py'),\n",
    "    data_files={\n",
    "        'test': os.path.abspath('../dataset_files/semeval_2014/{domain}_Test_Gold.xml'.format(domain=in_domain_dataset_name)),\n",
    "        'train': os.path.abspath('../dataset_files/semeval_2014/{domain}_Train_v2.xml'.format(domain=in_domain_dataset_name))\n",
    "    },\n",
    "    cache_dir='../dataset_cache')\n",
    "\n",
    "\n",
    "out_domain_semeval_dataset = datasets.load_dataset(\n",
    "    os.path.abspath('../dataset_scripts/semeval2014_task4/semeval2014_task4.py'),\n",
    "    data_files={\n",
    "        'test': os.path.abspath('../dataset_files/semeval_2014/{domain}_Test_Gold.xml'.format(domain=out_domain_dataset_name)),\n",
    "        'train': os.path.abspath('../dataset_files/semeval_2014/{domain}_Train_v2.xml'.format(domain=out_domain_dataset_name))\n",
    "    },\n",
    "    cache_dir='../dataset_cache')\n",
    "\n",
    "train_dataset = out_domain_semeval_dataset[\"train\"]\n",
    "val_dataset = in_domain_semeval_dataset[\"train\"]\n",
    "test_dataset = in_domain_semeval_dataset[\"test\"]\n",
    "\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82qv1NeYhb0B"
   },
   "source": [
    "## Append prompts to review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UII4cJtehb0F"
   },
   "outputs": [],
   "source": [
    "# Takes in batches from the dataset and makes an example for every prompt, text pair\n",
    "def add_prompts(reviews, prompts):\n",
    "\n",
    "    # Collect the output from each example in the batch\n",
    "    texts = []\n",
    "    sentiments = []\n",
    "    ids = []\n",
    "    aspect_prompts = []\n",
    "    \n",
    "    for i in range(len(reviews[\"aspect\"])):\n",
    "        \n",
    "        aspect = reviews[\"aspect\"][i]\n",
    "        text = reviews[\"text\"][i]\n",
    "        sentiment = reviews[\"sentiment\"][i]\n",
    "        \n",
    "        # ID to identify the review, aspect pair for regrouping later\n",
    "        review_aspect_id = str(uuid.uuid1())\n",
    "        \n",
    "        for p in prompts:\n",
    "            aspect_prompt = p.format(aspect=aspect)\n",
    "\n",
    "            texts.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "            ids.append(review_aspect_id)\n",
    "            aspect_prompts.append(aspect_prompt)\n",
    "\n",
    "    return {\"text\":texts, \"prompt\": aspect_prompts, \"label\": sentiments, \"review_aspect_id\": ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ip6umoSOhb0I",
    "outputId": "495f06b1-fd09-46ab-fdca-35676be626bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../dataset_cache\\sem_eval2014_task4_dataset\\default-790c778f2f732468\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969\\cache-6268c5a73dc375bd.arrow\n",
      "Loading cached processed dataset at ../dataset_cache\\sem_eval2014_task4_dataset\\default-3a5e3b6e10fdd547\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969\\cache-3c33b4c07a4b0f9e.arrow\n",
      "Loading cached processed dataset at ../dataset_cache\\sem_eval2014_task4_dataset\\default-3a5e3b6e10fdd547\\0.0.1\\f33ba7108331ad17be3f9fb710ca001edb383fba797c6ed0938354e6812ca969\\cache-acaa1f1ef2d50386.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'prompt': 'The Boot time is [MASK].', 'review_aspect_id': 'b052c13b-82b9-11eb-8344-7085c2c04498', 'text': 'Boot time is super fast, around anywhere from 35 seconds to 1 minute.'}\n",
      "2552\n"
     ]
    }
   ],
   "source": [
    "# Map to add_prompts\n",
    "train_prompt_dataset = train_dataset.map(\n",
    "    lambda e: add_prompts(e, sentiment_prompts),\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batched=True)\n",
    "\n",
    "val_prompt_dataset = val_dataset.map(\n",
    "    lambda e: add_prompts(e, sentiment_prompts),\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    batched=True)\n",
    "\n",
    "test_prompt_dataset = test_dataset.map(\n",
    "    lambda e: add_prompts(e, sentiment_prompts),\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    batched=True)\n",
    "\n",
    "print(test_prompt_dataset[0])\n",
    "print(len(test_prompt_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZMYmaM1hb0S"
   },
   "source": [
    "## Encode training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VS4M7AUmhb0Y"
   },
   "outputs": [],
   "source": [
    "# Tokenize every example and run it through the bert model\n",
    "# Outputs the last hidden state of the model\n",
    "def run_model(reviews, tokenizer, model, device):\n",
    "    \n",
    "    # Only the review text is truncated so that the mask token always exists in the tokens\n",
    "    batch_tokens = tokenizer(reviews[\"text\"], reviews[\"prompt\"], \n",
    "                             truncation='only_first', padding='max_length', max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "    batch_tokens.to(device=device)\n",
    "    \n",
    "    # Figures out where the mask token was placed\n",
    "    masked_indexes = []\n",
    "\n",
    "    for tokens_input_ids in batch_tokens.data[\"input_ids\"]:\n",
    "        masked_index = torch.nonzero(tokens_input_ids == tokenizer.mask_token_id, as_tuple=False).item()\n",
    "        masked_indexes.append(masked_index)\n",
    "    \n",
    "    # Run the batch through the model\n",
    "    outputs = model(**batch_tokens)\n",
    "    \n",
    "    # Extracts the last hidden states from the batch output\n",
    "    output_list = []\n",
    "    \n",
    "    for i in range(len(outputs[\"last_hidden_state\"])):\n",
    "        masked_index = masked_indexes[i]\n",
    "        output_list.append(outputs[\"last_hidden_state\"][i][masked_index])\n",
    "    \n",
    "    return {\"hidden_state\":output_list, \"label\": reviews[\"label\"], \"review_aspect_id\": reviews[\"review_aspect_id\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "799a9a16232445e09242864908de1904"
     ]
    },
    "id": "BzNSk0L6hb0Z",
    "outputId": "bd76c1e4-0008-4752-9bf8-0d7713fd1c3f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799a9a16232445e09242864908de1904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3602.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Maps the dataset using run_model\n",
    "train_model_output = train_prompt_dataset.map(\n",
    "    lambda e: run_model(e, tokenizer, model, torch_device),\n",
    "    remove_columns=train_prompt_dataset.column_names,\n",
    "    batched=True, batch_size=4, num_proc=None)\n",
    "\n",
    "val_model_output = val_prompt_dataset.map(\n",
    "    lambda e: run_model(e, tokenizer, model, torch_device),\n",
    "    remove_columns=train_prompt_dataset.column_names,\n",
    "    batched=True, batch_size=4, num_proc=None)\n",
    "\n",
    "test_model_output = test_prompt_dataset.map(\n",
    "    lambda e: run_model(e, tokenizer, model, torch_device),\n",
    "    remove_columns=train_prompt_dataset.column_names,\n",
    "    batched=True, batch_size=4, num_proc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUDvTSRhhb0g"
   },
   "outputs": [],
   "source": [
    "# Regroups the outputs by review, aspect pairs and concats the outputs into one long tensor\n",
    "def concat_tensors(examples, device):\n",
    "    # Make sure that everything in the batch is from the same review, aspect pair\n",
    "    ids = examples[\"review_aspect_id\"]\n",
    "    labels = examples[\"label\"]\n",
    "    \n",
    "    for a in ids:\n",
    "        for b in ids:\n",
    "            assert a == b\n",
    "            \n",
    "    for a in labels:\n",
    "        for b in labels:\n",
    "            assert a == b\n",
    "    \n",
    "    hidden_state_tensors = torch.FloatTensor(examples[\"hidden_state\"]).to(device=device)\n",
    "    cat_hs_tensor = torch.cat(tuple(hidden_state_tensors), 0).to(device=device)\n",
    "    \n",
    "    return {\"hidden_state\": [cat_hs_tensor], \"label\": [examples[\"label\"][0]], \"review_aspect_id\": [examples[\"review_aspect_id\"][0]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukZB5K5Rhb0h"
   },
   "outputs": [],
   "source": [
    "# Map to concat_tensors\n",
    "# Batch size has to be the same as sentiment prompts so that the examples in a batches all come from the same review\n",
    "train_hs_dataset = train_model_output.map(\n",
    "    lambda e: concat_tensors(e, torch_device),\n",
    "    remove_columns=train_model_output.column_names,\n",
    "    batched=True, batch_size=len(sentiment_prompts))\n",
    "\n",
    "val_hs_dataset = val_model_output.map(\n",
    "    lambda e: concat_tensors(e, torch_device),\n",
    "    remove_columns=val_model_output.column_names,\n",
    "    batched=True, batch_size=len(sentiment_prompts))\n",
    "\n",
    "test_hs_dataset = test_model_output.map(\n",
    "    lambda e: concat_tensors(e, torch_device),\n",
    "    remove_columns=test_model_output.column_names,\n",
    "    batched=True, batch_size=len(sentiment_prompts))\n",
    "\n",
    "#print(train_hs_dataset[0])\n",
    "print(len(train_hs_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TOMmAtIvoZ_"
   },
   "source": [
    "# Zero-shot ATSC with Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEIbN5Xthb0o"
   },
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9RniU9Lhb0s"
   },
   "outputs": [],
   "source": [
    "# This is the classification model that was trained to convert hidden state values to a class prediction\n",
    "class SentimentClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6i5tPiAhb0u"
   },
   "outputs": [],
   "source": [
    "# Output dataset to tensors from a dataloader\n",
    "train_hs_dataset.set_format(type='torch', columns=['hidden_state', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(train_hs_dataset, batch_size=32)\n",
    "\n",
    "val_hs_dataset.set_format(type='torch', columns=['hidden_state', 'label'])\n",
    "val_dataloader = torch.utils.data.DataLoader(val_hs_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uIwk07Rhb0w"
   },
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "epochs = 10\n",
    "\n",
    "# Bert hidden State size\n",
    "input_dim = 768 * len(sentiment_prompts)\n",
    "print(input_dim)\n",
    "output_dim = 3\n",
    "lr_rate = 0.0001\n",
    "\n",
    "classifier_model = SentimentClassifier(input_dim, output_dim)\n",
    "classifier_model.to(device=torch_device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier_model.parameters(), lr=lr_rate)\n",
    "\n",
    "for epoch in range(int(epochs)):\n",
    "    for batch in dataloader:\n",
    "        hidden_states = batch[\"hidden_state\"]\n",
    "        hidden_states = hidden_states.float().to(device=torch_device)\n",
    "        \n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        labels = labels.to(device=torch_device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_model(hidden_states)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model using val dataset\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_val in val_dataloader:\n",
    "\n",
    "        hidden_states = batch_val[\"hidden_state\"]\n",
    "        hidden_states = hidden_states.float().to(device=torch_device)\n",
    "\n",
    "        labels = batch_val[\"label\"]\n",
    "        labeles = labels.to(device=torch_device)\n",
    "\n",
    "        outputs = classifier_model(hidden_states)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total+= labels.size(0)\n",
    "\n",
    "        # for gpu, bring the predicted and labels back to cpu for python operations to work\n",
    "        predicted = predicted.to(device=\"cpu\")\n",
    "        labels = labels.to(device=\"cpu\")\n",
    "\n",
    "        correct+= (predicted == labels).sum()\n",
    "    accuracy = 100 * correct/total\n",
    "    print(\"Epoch: {}. Loss: {}. Validation Accuracy: {}.\".format(epoch, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l1H_XIPhb0y"
   },
   "source": [
    "## Run the classifier on hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLcc_wZjhb0y"
   },
   "outputs": [],
   "source": [
    "classifier_model.eval()\n",
    "test_hs_dataset.set_format(type='torch', columns=['hidden_state', 'label'])\n",
    "\n",
    "# Run the LR model on the dataset\n",
    "def make_predictions(examples, classifier_model, torch_device):\n",
    "    \n",
    "    cat_hs_tensor = examples[\"hidden_state\"].float().to(device=torch_device)\n",
    "    class_probs = classifier_model(cat_hs_tensor)\n",
    "\n",
    "    predictions = []\n",
    "    for cp in class_probs:\n",
    "        predictions.append(torch.argmax(cp))\n",
    "    \n",
    "    return {\"prediction\": predictions, \"label\": examples[\"label\"].tolist()}\n",
    "\n",
    "# Batch size has to be the same as sentiment prompts so that the examples in a batches all come from the same review\n",
    "predictions = test_hs_dataset.map(\n",
    "    lambda e: make_predictions(e, classifier_model, torch_device),\n",
    "    batched=True, batch_size=7, num_proc=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzqzXHsdhb00"
   },
   "source": [
    "## Convert prompt class probs to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtbOrKo7xgg6"
   },
   "outputs": [],
   "source": [
    "# This regroups the models by the review_aspect_id\n",
    "# and makes a prediction based upon each groups class prob output from the LR model\n",
    "def make_predictions(example, device):\n",
    "    ids = example[\"review_aspect_id\"]\n",
    "    labels = example[\"label\"]\n",
    "    for a in ids:\n",
    "        for b in ids:\n",
    "            assert a == b\n",
    "            \n",
    "    for a in labels:\n",
    "        for b in labels:\n",
    "            assert a == b\n",
    "            \n",
    "    class_probs = torch.FloatTensor(example[\"class_probs\"])\n",
    "    prediction_mean = torch.mean(class_probs, 0)\n",
    "    prediction = torch.argmax(prediction_mean)\n",
    "    \n",
    "    return {\"prediction\": [prediction], \"label\": [example[\"label\"][0]], \"review_aspect_id\": [example[\"review_aspect_id\"][0]]}\n",
    "\n",
    "predictions = LR_output.map(\n",
    "    lambda e: make_predictions(e, torch_device),\n",
    "    remove_columns=LR_output.column_names,\n",
    "    batched=True, batch_size=len(sentiment_prompts), num_proc=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjpA_0m1hb08"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9G9AUeQhb09"
   },
   "outputs": [],
   "source": [
    "# Calculate metrics and confusion matrix based upon predictions and true labels\n",
    "accuracy = accuracy_score(predictions[\"label\"], predictions[\"prediction\"])\n",
    "\n",
    "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
    "\n",
    "cm = confusion_matrix(predictions[\"label\"], predictions[\"prediction\"])\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"positive\", \"negative\", \"neutral\"]],\n",
    "                  columns = [i for i in [\"positive\", \"negative\", \"neutral\"]])\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "ax = sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU3E4AiYhb0_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MLM_eval.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
