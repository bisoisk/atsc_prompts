{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GvsqHtXQvvX"
   },
   "source": [
    "# Initial Setups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTrRCzgmFwYn"
   },
   "source": [
    "## (Google Colab use only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xC1S4mWFFv5U"
   },
   "outputs": [],
   "source": [
    "# Use Google Colab\n",
    "use_colab = True\n",
    "\n",
    "# Is this notebook running on Colab?\n",
    "# If so, then google.colab package (github.com/googlecolab/colabtools)\n",
    "# should be available in this environment\n",
    "\n",
    "# Previous version used importlib, but we could do the same thing with\n",
    "# just attempting to import google.colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    colab_available = True\n",
    "except:\n",
    "    colab_available = False\n",
    "\n",
    "if use_colab and colab_available:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # If there's a package I need to install separately, do it here\n",
    "    #!pip install pyro-ppl\n",
    "\n",
    "    # cd to the appropriate working directory under my Google Drive\n",
    "    %cd 'drive/My Drive/cs696ds_lexalytics/Language Model Finetuning'\n",
    "    \n",
    "    # Install packages specified in requirements\n",
    "    !pip install -r requirements.txt\n",
    "    \n",
    "    # List the directory contents\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rgITVrTUZstB"
   },
   "outputs": [],
   "source": [
    "# IPython reloading magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZ5SZXkbEp34"
   },
   "source": [
    "## Experiment ID\n",
    "\n",
    "**NOTE**: The following `experiment_id` MUST BE CHANGED in order to avoid overwriting the files from other experiments!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rUqrV6VeEs3Z"
   },
   "outputs": [],
   "source": [
    "# We will use the following string ID to identify this particular (training) experiments\n",
    "# in directory paths and other settings\n",
    "experiment_id = 'lm_further_pretraining_bert_yelp_restaurants'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSItEk35-R8o"
   },
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "o-jSRWQfLL4U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.10 (default, Feb 26 2021, 18:47:35) \n",
      "[GCC 7.3.0]\n",
      "NumPy version: 1.19.2\n",
      "PyTorch version: 1.7.1\n",
      "Transformers version: 4.3.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "import utils\n",
    "\n",
    "# Random seed settings\n",
    "random_seed = 696\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Print version information\n",
    "print(\"Python version: \" + sys.version)\n",
    "print(\"NumPy version: \" + np.__version__)\n",
    "print(\"PyTorch version: \" + torch.__version__)\n",
    "print(\"Transformers version: \" + transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkKDoXUp-UIi"
   },
   "source": [
    "## PyTorch GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "je9BT2pQIpUx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 11.0\n",
      "cuDNN Version: 8005\n",
      "CUDA Device Name: GeForce GTX TITAN X\n",
      "CUDA Capabilities: (5, 2)\n",
      "\n",
      "PyTorch device selected: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # https://github.com/UKPLab/sentence-transformers/issues/413\n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "    # https://github.com/pytorch/pytorch/issues/37377\n",
    "    os.environ['MKL_SERVICE_FORCE_INTEL'] = \"1\"\n",
    "    \n",
    "    torch_device = torch.device('cuda')\n",
    "\n",
    "    # Set this to True to make your output immediately reproducible\n",
    "    # Note: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    \n",
    "    # Disable 'benchmark' mode: Set this False if you want to measure running times more fairly\n",
    "    # Note: https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Faster Host to GPU copies with page-locked memory\n",
    "    use_pin_memory = True \n",
    "\n",
    "    # CUDA libraries version information\n",
    "    print(\"CUDA Version: \" + str(torch.version.cuda))\n",
    "    print(\"cuDNN Version: \" + str(torch.backends.cudnn.version()))\n",
    "    print(\"CUDA Device Name: \" + str(torch.cuda.get_device_name()))\n",
    "    print(\"CUDA Capabilities: \"+ str(torch.cuda.get_device_capability()))\n",
    "else:\n",
    "    torch_device = torch.device('cpu')\n",
    "    use_pin_memory = False\n",
    "\n",
    "print()\n",
    "print(\"PyTorch device selected:\", torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3txs5s7Q1UG"
   },
   "source": [
    "# Further pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEnUsBDUOLAm"
   },
   "source": [
    "## Load the BERT-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "W3TCqS-3OOIj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir='./bert_base_cache')\n",
    "model = transformers.AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\", cache_dir='./bert_base_cache')\n",
    "\n",
    "model = model.to(torch_device)\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrLTLsRFRUKK"
   },
   "source": [
    "## Load the Yelp restaurants dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZpKL0urORmkm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset yelp_restaurants/default-ceef9e04fb6ab232 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to ./dataset_cache/yelp_restaurants/default-ceef9e04fb6ab232/0.0.0/1a6412d257d1f098c2e7a0ac3a450b82eb6f35095fa78cc3ff87d9a1b65368f4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset yelp_restaurants downloaded and prepared to ./dataset_cache/yelp_restaurants/default-ceef9e04fb6ab232/0.0.0/1a6412d257d1f098c2e7a0ac3a450b82eb6f35095fa78cc3ff87d9a1b65368f4. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "yelp = datasets.load_dataset(\n",
    "    './dataset_scripts/yelp_restaurants',\n",
    "    data_files={\n",
    "        'train': 'dataset_files/yelp_restaurants/yelp_academic_dataset_review.json',\n",
    "        'restaurant_ids': 'dataset_files/yelp_restaurants/restaurantIDs.txt',\n",
    "    },\n",
    "    cache_dir='./dataset_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2m9wZvndVCvy"
   },
   "outputs": [],
   "source": [
    "data_yelp_train = yelp['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WrB-ZoTz5-zb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 2152007\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training data:\", len(data_yelp_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sQ_RaowTYC9X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'EUgMR__LDQYjdD6qlWgrlw', 'stars': 1.0, 'text': \"Burgers is bland and tasteless, relies on toppings. So expensive. Hot dog is tiny and terribly salty. Fries are similar to Harveys, don't compare to New York Fries. Would not go back.\"}\n"
     ]
    }
   ],
   "source": [
    "# Check out how individual data points look like\n",
    "print(data_yelp_train[696])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqyEwcDGSJjs"
   },
   "source": [
    "### Preprocessing: Encode the text with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tTVxGSY4oe7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset_pretraining = data_yelp_train.map(\n",
    "    lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=256),\n",
    "    remove_columns=data_yelp_train.column_names,\n",
    "    batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1N4PUdHZeIm"
   },
   "source": [
    "## Pre-train further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD6lfHh8mURq"
   },
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RFho5vyJgVaj"
   },
   "outputs": [],
   "source": [
    "# Our new 'smart' masking\n",
    "collator = utils.DataCollatorForSmartMLM(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    "    prob_replace_with_mask=0.8,\n",
    "    prob_replace_with_random=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8ps4XzQxmTgS"
   },
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=os.path.join('.', 'progress', experiment_id, 'results'), # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=32,  # 32 * 4 GPUs = 128 Total\n",
    "    evaluation_strategy='epoch',\n",
    "    warmup_steps=5000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=os.path.join('.', 'progress', experiment_id, 'logs'), # directory for storing logs\n",
    "    seed=random_seed,\n",
    "    fp16=True,\n",
    "    fp16_opt_level='O2',\n",
    "    fp16_backend='amp',\n",
    "    prediction_loss_only=True,\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RQsRp9lMZstE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(training_args.n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1OQJ8IOIYHXb"
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator, # do the masking on the go\n",
    "    train_dataset=train_dataset_pretraining,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTx5562OmXx5"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-_v3lAPvb9DK"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/bseoh/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/bseoh/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/mnt/nfs/work1/696ds-s21/bseoh/lexalytics/utils/data_collator_smart_mlm.py\", line 80, in __call__\n    batch = self.tokenizer.pad(examples, return_tensors=\"pt\")\n  File \"/home/bseoh/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2641, in pad\n    \"You should supply an encoding or a list of encodings to this method\"\nValueError: You should supply an encoding or a list of encodings to this methodthat includes input_ids, but you provided []\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/bseoh/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/bseoh/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/mnt/nfs/work1/696ds-s21/bseoh/lexalytics/utils/data_collator_smart_mlm.py\", line 80, in __call__\n    batch = self.tokenizer.pad(examples, return_tensors=\"pt\")\n  File \"/home/bseoh/miniconda3/envs/zeroshotatsc/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2641, in pad\n    \"You should supply an encoding or a list of encodings to this method\"\nValueError: You should supply an encoding or a list of encodings to this methodthat includes input_ids, but you provided []\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLj4Ico8vwhO"
   },
   "source": [
    "### Save the model to the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLViPwdTvvxP"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join('.', 'trained_models', experiment_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIQn4r1oVJp6"
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(os.path.join('.', 'trained_models', experiment_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNUCURCduqYa"
   },
   "source": [
    "## LM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1XN8ot3us18"
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKkBZEpF07Ip"
   },
   "outputs": [],
   "source": [
    "print(eval_results)\n",
    "\n",
    "perplexity = np.exp(eval_results[\"eval_loss\"])\n",
    "\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_bAx1rL0KFb"
   },
   "source": [
    "## Playing with my own input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmIJszldveH6"
   },
   "outputs": [],
   "source": [
    "example = f\"\"\"The {tokenizer.mask_token} of {tokenizer.mask_token} is awful, but its {tokenizer.mask_token} is fantastic.\"\"\"\n",
    "\n",
    "example_encoded = tokenizer.encode(example, add_special_tokens=True, return_tensors=\"pt\").to(torch_device)\n",
    "\n",
    "# Let's decode this back just to see how they were actually encoded\n",
    "example_tokens = []\n",
    "\n",
    "for id in example_encoded[0]:\n",
    "    example_tokens.append(tokenizer.convert_ids_to_tokens(id.item()))\n",
    "\n",
    "print(example_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbHup4-rxBSD"
   },
   "outputs": [],
   "source": [
    "example_prediction = model(example_encoded)\n",
    "\n",
    "example_prediction_argmax = torch.argmax(example_prediction[0], dim=-1)[0]\n",
    "\n",
    "print(example_prediction_argmax)\n",
    "\n",
    "print(tokenizer.decode(example_prediction_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EM3YetZAm3L-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lm_further_pretraining_bert_yelp_restaurants.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
